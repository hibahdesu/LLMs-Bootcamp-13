{"cells":[{"cell_type":"markdown","metadata":{"id":"F4sJ3lTuLRYk"},"source":["# Model Fine Tuning\n","----\n","**Objective**: In this notebook, you will get to fine tune a transformer model on MRPC dataset. You will get to go through the whole process of transfer learning from loading the dataset, loading the model, compiling the model, training the model, and finally evaluating your results.\n","\n","NOTE: Make sure to change the runtime from CPU to TPU or GPU for faster training"]},{"cell_type":"markdown","metadata":{"id":"T4nodfN4LRYl"},"source":["## Install Libraries\n","Install the Transformers, Datasets, and Evaluate libraries to run this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yz8FzdJyLRYm"},"outputs":[],"source":["!pip install datasets evaluate transformers[sentencepiece]"]},{"cell_type":"markdown","source":["## Import Dataset\n","___\n","In this section we will use as an example the MRPC (Microsoft Research Paraphrase Corpus) dataset. The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing).\n","This is one of the 10 datasets composing the GLUE benchmark, which is an academic benchmark that is used to measure the performance of ML models across 10 different text classification tasks.\n"],"metadata":{"id":"JtHyWhAFQfKV"}},{"cell_type":"markdown","source":["**Question 1**: Use the \"load_datasets\" function to load the \"mrpc\" dataset from the \"glue\" benchmark."],"metadata":{"id":"wmsCwdML8-S1"}},{"cell_type":"code","source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","\n","raw_datasets = ----- # Add your solution here"],"metadata":{"id":"NBH39H2RQ2sl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pre-processing data"],"metadata":{"id":"oP0ciMkWRJZZ"}},{"cell_type":"markdown","source":["First, we need to tokenize our data. We will do so using the BERT tokenizer. We will use the AutoTokenizer with truncation.\n","This will truncate token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a batch of pairs) is provided.\n","\n","**Question 2:** Use the .map() function to apply the tokenization functionon the raw dataset. Hint: set the batched flag to True for faster processing."],"metadata":{"id":"xoOv8gsnYtoE"}},{"cell_type":"code","source":["checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","\n","def tokenize_function(example):\n","    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n","\n","\n","tokenized_datasets = raw_datasets.---- # Add your solution here"],"metadata":{"id":"Hax0Y98_YtHA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then, we need to pad our data so that all input sequences have the same lengths."],"metadata":{"id":"-U7C7dImY05M"}},{"cell_type":"code","source":["data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"],"metadata":{"id":"Bj_tzWleY8fo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lets take a training example to see what our dataset looks like."],"metadata":{"id":"7Khl3lsEw8Eh"}},{"cell_type":"markdown","source":["**Question 3:** Display an example from the training section of the dataset. Hint: The tokenized_datasets variable is a nested dictionary where the main keys are \"train\", \"test\", and \"validate\""],"metadata":{"id":"T3Dg4yMVJgjT"}},{"cell_type":"code","source":["-----  # Add your solution here"],"metadata":{"id":"yV-mxLZntqF2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, we will prepare our training and validation datasets by transforming them into tensorflow datasets."],"metadata":{"id":"wI57YOQFZD1I"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qn3MiZBhLRYm"},"outputs":[],"source":["tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n","    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n","    label_cols=[\"labels\"],\n","    shuffle=True,\n","    collate_fn=data_collator,\n","    batch_size=8,\n",")\n","\n","tf_validation_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n","    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n","    label_cols=[\"labels\"],\n","    shuffle=False,\n","    collate_fn=data_collator,\n","    batch_size=8,\n",")\n","\n","tf_test_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n","    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n","    label_cols=[\"labels\"],\n","    shuffle=False,\n","    collate_fn=data_collator,\n","    batch_size=8,\n",")"]},{"cell_type":"markdown","source":["## Model Preperation\n","---\n","In this section, we will load the model and equip it with a classifier head. This model is already pre-trained and we will fine tune it with the dataset for paraphrasing classification that we prepared."],"metadata":{"id":"coK-FUIJZJN7"}},{"cell_type":"markdown","source":["### Load Classifier Head"],"metadata":{"id":"NI82QYa_ZyaY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7p2oTOYWLRYn"},"outputs":[],"source":["from transformers import TFAutoModelForSequenceClassification\n","\n","model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"]},{"cell_type":"markdown","source":["### Compile the model"],"metadata":{"id":"J1M9koMdZtBY"}},{"cell_type":"markdown","source":["We will compile the model using SparseCategoricalCrossentropyloss, which is when there are two or more label classes that are not one hot encded."],"metadata":{"id":"xBuP4OkCnjSH"}},{"cell_type":"markdown","source":["**Question 4:** Compile the model using the adam optimizer, SparseCategoricalCrossentropy loss (with from_logits flag set to True) and displaying the accuracy metric."],"metadata":{"id":"rpsUmQJ7J0rQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nZpWdSH1LRYn"},"outputs":[],"source":["from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","\n","# Add your solution here\n"]},{"cell_type":"markdown","source":["### Train the model"],"metadata":{"id":"zLbraPdeZ4Nw"}},{"cell_type":"markdown","source":["Now we will train our model using the processing training and validation sets."],"metadata":{"id":"WU3gnhjjn1pl"}},{"cell_type":"markdown","source":["**Question 5:** Fit the model on the training and validation sections that we prepared earlier."],"metadata":{"id":"dD-b6MXaKF_3"}},{"cell_type":"code","source":["# Add your solution here"],"metadata":{"id":"i60lj4LhZ3sb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluating the model"],"metadata":{"id":"7Yt0L5JCeAIs"}},{"cell_type":"markdown","source":["Let's test how accurate the models are! In this section, we will predict on the testing dataset to check the model's performance when it comes to classifying paraphrased sentences on unseen data."],"metadata":{"id":"B-JGZJUOn7Oa"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nd9u7KIdLRYo"},"outputs":[],"source":["preds = model.predict(tf_test_dataset)[\"logits\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gk91_I_ELRYo"},"outputs":[],"source":["import numpy as np\n","class_preds = np.argmax(preds, axis=1)\n","print(preds.shape, class_preds.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LnlD4xCLLRYo"},"outputs":[],"source":["import evaluate\n","\n","metric = evaluate.load(\"glue\", \"mrpc\")\n","metric.compute(predictions=class_preds, references=raw_datasets[\"test\"][\"label\"])"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}