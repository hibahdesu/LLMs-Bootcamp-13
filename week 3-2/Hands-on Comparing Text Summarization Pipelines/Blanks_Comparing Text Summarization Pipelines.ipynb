{"cells":[{"cell_type":"markdown","source":["# Comparing Text Summarization Pipelines\n","\n","---\n","\n","Objective: In this notebook, we will load pre-trained summarization models from the transformers library provided from huggingface. The goal is to get you familiar with loading Large Language Models for any use case."],"metadata":{"id":"ltapB3ljkNan"}},{"cell_type":"markdown","source":["## Installing Dependecies\n","----\n","To get started, we need to install the datasets and transformers libraries in our instance. We use the pip library to be able to do so."],"metadata":{"id":"uC14Moz0kpfW"}},{"cell_type":"code","source":["!pip install datasets==2.0.0\n","!pip install transformers"],"metadata":{"id":"52pt9v_5B-qp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h0_FVYjPBuM0"},"source":["## Loading the Dataset\n","---\n","For this notebook, we will be using the CNN/DailyMail Dataset. The CNN / DailyMail Dataset is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail. The current version supports both extractive and abstractive summarization, though the original version was created for machine reading and comprehension and abstractive question answering."]},{"cell_type":"markdown","metadata":{"id":"cck2Oa4cBuM0"},"source":["### Loading the Dataset\n","\n","---\n","We need to load the 3.0.0 version of the 'cnn_dailymail' dataset from the datasets library."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_kQSpylGBuM0"},"outputs":[],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"---\", version=\"----\") #add the model name then model version"]},{"cell_type":"markdown","source":["### Visualizing a sample\n","\n","---\n","Let's visualize a sample from our dataset to get a good look at what it looks like. We will visulaize the first 500 charachters of it."],"metadata":{"id":"ZWP3_wuXmkxC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VrQejNrgBuM1"},"outputs":[],"source":["sample = dataset[\"train\"][-] #enter the index of the first element\n","\n","#Printing the article\n","print(f\"\"\"\n","Article (excerpt of 500 characters, total length: {len(sample[\"article\"])}):\n","\"\"\")\n","print(sample[\"article\"][---]) #enter the slice that would take the first 500 charachters of a string\n","\n","#Printing the summary\n","print(f'\\nSummary (length: {len(sample[\"highlights\"])}):')\n","print(sample[\"highlights\"])"]},{"cell_type":"markdown","metadata":{"id":"24Af02R1BuM1"},"source":["## Text Summarization Pipelines\n","---\n","In this section, we will be loading the state of the art models that we will compare with.\n","\n","We will be infering on a sample text from the dataset and storing it in the sample_text variable.\n","\n","We'll collect the generated summaries of each model in a dictionary called summaries."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6swW98J5BuM1"},"outputs":[],"source":["sample_text = dataset[\"train\"][1][\"article\"][:2000]\n","summaries = {}"]},{"cell_type":"markdown","metadata":{"id":"edDEKX-NBuM1"},"source":["### Summarization Baseline\n","---\n","As a baseline, we will be using the sent_tokenize function from the NLTK library. The sent_tokenize function uses an instance of PunktSentenceTokenizer from the nltk. tokenize. punkt module, which is already been trained and thus very well knows to mark the end and beginning of sentence at what characters and punctuation. We will store its output in \"baseline\".\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_5sJf3iFBuM1"},"outputs":[],"source":["import nltk\n","from nltk.tokenize import sent_tokenize\n","\n","nltk.download(\"punkt\")"]},{"cell_type":"markdown","source":["We will define a function that gives us a three sentence summary of the input text by taking the first 3 token outputs."],"metadata":{"id":"aMK8Y1gQoGGa"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fNCIC-x6BuM1"},"outputs":[],"source":["def three_sentence_summary(text):\n","    return \"\\n\".join(sent_tokenize(text)[:3])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2oLTt2FLBuM1"},"outputs":[],"source":["summaries[\"baseline\"] = three_sentence_summary(sample_text)"]},{"cell_type":"markdown","metadata":{"id":"LOpvp_vJBuM1"},"source":["### Pegasus\n","---\n","The Pegasus model was proposed in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Google. Pegasusâ€™ pretraining task is intentionally similar to summarization: important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary.\n","\n","We will import the \"pegasus_summarizer\" model from the huggingface user turner007."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3y_rVT88BuM1"},"outputs":[],"source":["#hide_output\n","from transformers import pipeline, set_seed\n","\n","set_seed(42) #random seed for reproducibility\n","pipe = pipeline(\"summarization\", model=\"turner007/---\") #enter model name\n","gpt2_query = sample_text + \"\\nTL;DR:\\n\"\n","pipe_out = pipe(gpt2_query, max_length=512, clean_up_tokenization_spaces=True)\n","summaries[\"gpt2\"] = \"\\n\".join(\n","    sent_tokenize(pipe_out[0][\"generated_text\"]))"]},{"cell_type":"markdown","metadata":{"id":"WdZ1PKfCBuM2"},"source":["### BART\n","---\n","The Bart model was proposed in the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. We can call the summarization fine tuned model which is called \"facebook/bart-large-cnn\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xOPuPD_rBuM2"},"outputs":[],"source":["pipe = pipeline(\"---\", model=\"---\")\n","pipe_out = pipe(sample_text)\n","summaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))"]},{"cell_type":"markdown","metadata":{"id":"JF1pGNmZBuM2"},"source":["## Comparing Different Summaries"]},{"cell_type":"markdown","source":["Now, we will compare the different summaries generated by the models."],"metadata":{"id":"NQD5_YNBq4iU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z27Hr1OvBuM2"},"outputs":[],"source":["print(\"ORIGINAL TEXT:\")\n","print(dataset[\"train\"][1][\"highlights\"])\n","print(\"\")\n","\n","for model_name in summaries:\n","    print(model_name.upper())\n","    print(summaries[model_name])\n","    print(\"SUMMARY LENGTH: \", len(summaries[model_name]))\n","    print(\"\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"colab":{"provenance":[]},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":0}