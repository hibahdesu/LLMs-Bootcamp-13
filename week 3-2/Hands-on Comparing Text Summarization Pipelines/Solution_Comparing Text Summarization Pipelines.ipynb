{"cells":[{"cell_type":"markdown","source":["# Comparing Text Summarization Pipelines\n","\n","---\n","\n","Objective: In this notebook, we will load pre-trained summarization models from the transformers library provided from huggingface. The goal is to get you familiar with loading Large Language Models for any use case."],"metadata":{"id":"ltapB3ljkNan"}},{"cell_type":"markdown","source":["## Installing Dependecies\n","----\n","To get started, we need to install the datasets and transformers libraries in our instance. We use the pip library to be able to do so."],"metadata":{"id":"uC14Moz0kpfW"}},{"cell_type":"code","source":["!pip install datasets==2.0.0\n","!pip install transformers\n","!pip install sentencepiece"],"metadata":{"id":"52pt9v_5B-qp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h0_FVYjPBuM0"},"source":["## Loading the Dataset\n","---\n","For this notebook, we will be using the CNN/DailyMail Dataset. The CNN / DailyMail Dataset is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail. The current version supports both extractive and abstractive summarization, though the original version was created for machine reading and comprehension and abstractive question answering."]},{"cell_type":"markdown","metadata":{"id":"cck2Oa4cBuM0"},"source":["### Loading the Dataset\n","\n","---\n","We need to load the 'cnn_dailymail' dataset from the datasets library."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_kQSpylGBuM0","outputId":"ba812fc2-28a8-4191-a523-3a21d71ba9c3","colab":{"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["705345daa72a4784bcad06cfaf6fbdaa","4cc28d790ec54feea2056d1e29e911cb","d9eb0462a77447c2b4bef3ea2794deb7","6f152da4af41488fbc6247672ab83e35","128ecaca4e574c9192ebcda8b01af75d","294d663f18104ba4a85ed71699376f0e","3c56672ad2d74fe1805ba748f28c2d92","72901ed1938145779a0543e9bde44203","06f94e08569c4ef1a1992c4cef21eca8","edf9e80aef9c4a94915db5d6dbc1c979","1fd04067366646d39f94252fe4249f1f"]},"executionInfo":{"status":"ok","timestamp":1692198086824,"user_tz":-180,"elapsed":1224,"user":{"displayName":"Jana Kibrit","userId":"13986201213012064329"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.builder:Using custom data configuration default\n","WARNING:datasets.builder:Reusing dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/default/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"705345daa72a4784bcad06cfaf6fbdaa"}},"metadata":{}}],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"cnn_dailymail\", version=\"3.0.0\")"]},{"cell_type":"markdown","source":["### Visualizing a sample\n","\n","---\n","Let's visualize a sample from our dataset to get a good look at what it looks like. We will visulaize the first 500 charachters of it."],"metadata":{"id":"ZWP3_wuXmkxC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"VrQejNrgBuM1","outputId":"c9fc52a7-4ca7-4a3f-af98-6598914884f9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692198086824,"user_tz":-180,"elapsed":13,"user":{"displayName":"Jana Kibrit","userId":"13986201213012064329"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Article (excerpt of 500 characters, total length: 9396):\n","\n","It's official: U.S. President Barack Obama wants lawmakers to weigh in on whether to use military force in Syria. Obama sent a letter to the heads of the House and Senate on Saturday night, hours after announcing that he believes military action against Syrian targets is the right step to take over the alleged use of chemical weapons. The proposed legislation from Obama asks Congress to approve the use of military force \"to deter, disrupt, prevent and degrade the potential for future uses of che\n","\n","Summary (length: 294):\n","Syrian official: Obama climbed to the top of the tree, \"doesn't know how to get down\"\n","Obama sends a letter to the heads of the House and Senate .\n","Obama to seek congressional approval on military action against Syria .\n","Aim is to determine whether CW were used, not by whom, says U.N. spokesman .\n"]}],"source":["sample = dataset[\"train\"][0]\n","\n","#Printing the article\n","print(f\"\"\"\n","Article (excerpt of 500 characters, total length: {len(sample[\"article\"])}):\n","\"\"\")\n","print(sample[\"article\"][:500])\n","\n","#Printing the summary\n","print(f'\\nSummary (length: {len(sample[\"highlights\"])}):')\n","print(sample[\"highlights\"])"]},{"cell_type":"markdown","metadata":{"id":"24Af02R1BuM1"},"source":["## Text Summarization Pipelines\n","---\n","In this section, we will be loading the state of the art models that we will compare with.\n","\n","We will be infering on a sample text from the dataset and storing it in the sample_text variable.\n","\n","We'll collect the generated summaries of each model in a dictionary called summaries."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6swW98J5BuM1"},"outputs":[],"source":["sample_text = dataset[\"train\"][1][\"article\"][:2000]\n","summaries = {}"]},{"cell_type":"markdown","metadata":{"id":"edDEKX-NBuM1"},"source":["### Summarization Baseline\n","---\n","As a baseline, we will be using the sent_tokenize function from the NLTK library. The sent_tokenize function uses an instance of PunktSentenceTokenizer from the nltk. tokenize. punkt module, which is already been trained and thus very well knows to mark the end and beginning of sentence at what characters and punctuation. We will store its output in \"baseline\".\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_5sJf3iFBuM1","outputId":"fde8e6ec-8a17-4581-e965-b04aea0b00ee","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692198086825,"user_tz":-180,"elapsed":10,"user":{"displayName":"Jana Kibrit","userId":"13986201213012064329"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":26}],"source":["import nltk\n","from nltk.tokenize import sent_tokenize\n","\n","nltk.download(\"punkt\")"]},{"cell_type":"markdown","source":["We will define a function that gives us a three sentence summary of the input text by taking the first 3 token outputs."],"metadata":{"id":"aMK8Y1gQoGGa"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fNCIC-x6BuM1"},"outputs":[],"source":["def three_sentence_summary(text):\n","    return \"\\n\".join(sent_tokenize(text)[:3])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2oLTt2FLBuM1"},"outputs":[],"source":["summaries[\"baseline\"] = three_sentence_summary(sample_text)"]},{"cell_type":"markdown","metadata":{"id":"LOpvp_vJBuM1"},"source":["### Pegasus\n","---\n","The Pegasus model was proposed in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization by Google. Pegasusâ€™ pretraining task is intentionally similar to summarization: important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary.\n","\n","We will import the \"pegasus_summarizer\" model from the huggingface user turner007."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3y_rVT88BuM1"},"outputs":[],"source":["from transformers import pipeline, set_seed\n","\n","set_seed(42) #random seed for reproducibility\n","pipe = pipeline(\"summarization\", model=\"tuner007/pegasus_summarizer\")\n","pegasus_query = sample_text + \"\\nTL;DR:\\n\"\n","pipe_out = pipe(pegasus_query, max_length=400, clean_up_tokenization_spaces=True)\n","summaries[\"pegasus\"] = \"\\n\".join(\n","    sent_tokenize(pipe_out[0][\"summary_text\"]))"]},{"cell_type":"markdown","metadata":{"id":"WdZ1PKfCBuM2"},"source":["### BART\n","---\n","The Bart model was proposed in the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xOPuPD_rBuM2"},"outputs":[],"source":["pipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n","pipe_out = pipe(sample_text)\n","summaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))"]},{"cell_type":"markdown","metadata":{"id":"JF1pGNmZBuM2"},"source":["## Comparing Different Summaries"]},{"cell_type":"markdown","source":["Now, we will compare the different summaries generated by the models."],"metadata":{"id":"NQD5_YNBq4iU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z27Hr1OvBuM2","outputId":"f0cb99bc-23d2-4209-a1e7-aeff46f13a53","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692198231220,"user_tz":-180,"elapsed":17,"user":{"displayName":"Jana Kibrit","userId":"13986201213012064329"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["ORIGINAL TEXT:\n","Usain Bolt wins third gold of world championship .\n","Anchors Jamaica to 4x100m relay victory .\n","Eighth gold at the championships for Bolt .\n","Jamaica double up in women's 4x100m relay .\n","\n","BASELINE\n","(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay.\n","The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds.\n","The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover.\n","SUMMARY LENGTH:  473\n","\n","PEGASUS\n","Jamaica's Usain Bolt won his third gold medal at the world championships on Sunday as he anchored his team to victory in the men's 4x100m relay.\n","The Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds.\n","The US finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover.\n","SUMMARY LENGTH:  373\n","\n","BART\n","Usain Bolt wins his third gold of the world championships in Moscow.\n","Bolt anchors Jamaica to victory in the men's 4x100m relay.\n","The 26-year-old has now won eight gold medals at world championships.\n","Jamaica's women also win gold in the relay, beating France in the process.\n","SUMMARY LENGTH:  272\n","\n"]}],"source":["print(\"ORIGINAL TEXT:\")\n","print(dataset[\"train\"][1][\"highlights\"])\n","print(\"\")\n","\n","for model_name in summaries:\n","    print(model_name.upper())\n","    print(summaries[model_name])\n","    print(\"SUMMARY LENGTH: \", len(summaries[model_name]))\n","    print(\"\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"colab":{"provenance":[]},"accelerator":"TPU","widgets":{"application/vnd.jupyter.widget-state+json":{"705345daa72a4784bcad06cfaf6fbdaa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4cc28d790ec54feea2056d1e29e911cb","IPY_MODEL_d9eb0462a77447c2b4bef3ea2794deb7","IPY_MODEL_6f152da4af41488fbc6247672ab83e35"],"layout":"IPY_MODEL_128ecaca4e574c9192ebcda8b01af75d"}},"4cc28d790ec54feea2056d1e29e911cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_294d663f18104ba4a85ed71699376f0e","placeholder":"â€‹","style":"IPY_MODEL_3c56672ad2d74fe1805ba748f28c2d92","value":"100%"}},"d9eb0462a77447c2b4bef3ea2794deb7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_72901ed1938145779a0543e9bde44203","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_06f94e08569c4ef1a1992c4cef21eca8","value":3}},"6f152da4af41488fbc6247672ab83e35":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_edf9e80aef9c4a94915db5d6dbc1c979","placeholder":"â€‹","style":"IPY_MODEL_1fd04067366646d39f94252fe4249f1f","value":" 3/3 [00:00&lt;00:00,  2.52it/s]"}},"128ecaca4e574c9192ebcda8b01af75d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"294d663f18104ba4a85ed71699376f0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c56672ad2d74fe1805ba748f28c2d92":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72901ed1938145779a0543e9bde44203":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06f94e08569c4ef1a1992c4cef21eca8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"edf9e80aef9c4a94915db5d6dbc1c979":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fd04067366646d39f94252fe4249f1f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}