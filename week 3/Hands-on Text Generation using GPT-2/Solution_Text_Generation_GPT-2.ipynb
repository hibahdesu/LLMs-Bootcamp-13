{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMS0jiNznUzU2Dd9Y7Tu0Fu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Text Generation using GPT-2 Language Model\n","\n","This notebook demonstrates text generation using the GPT-2 (Generative Pre-trained Transformer 2) language model, a state-of-the-art autoregressive language model developed by OpenAI. The notebook showcases how to utilize the 'transformers' library in Python to interact with GPT-2 and generate coherent and contextually relevant text based on a given prompt."],"metadata":{"id":"L9KNg9aZrDml"}},{"cell_type":"markdown","source":["## Importing Libraries\n","We start by importing the necessary libraries, including the 'transformers' library which provides access to pre-trained language models like GPT-2.\n","\n","The pre-trained GPT-2 model and tokenizer are loaded using the 'GPT2LMHeadModel' and 'GPT2Tokenizer' classes from the 'transformers' library. The GPT-2 model is set up for text generation."],"metadata":{"id":"av9r-C32rMPY"}},{"cell_type":"code","source":["from transformers import GPT2LMHeadModel, GPT2Tokenizer\n"],"metadata":{"id":"iLnoKMQ9psvl","executionInfo":{"status":"ok","timestamp":1690970359607,"user_tz":-180,"elapsed":3750,"user":{"displayName":"Amanda Hachem","userId":"07936363838080570525"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## Define the Text Generation Function\n","\n","A function named 'generate_text_with_gpt2()' is defined, which takes a prompt as input and uses the GPT-2 model to generate text based on the prompt. The function encodes the prompt, generates text using the GPT-2 model, and decodes the output to obtain human-readable text."],"metadata":{"id":"tRQFrOedrtJP"}},{"cell_type":"code","source":["\n","def generate_text_with_gpt2(prompt, max_length=100):\n","    # Load the pre-trained GPT-2 model and tokenizer\n","    model_name = \"gpt2\"\n","    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","    model = GPT2LMHeadModel.from_pretrained(model_name)\n","\n","    # Encode the prompt and generate text using GPT-2\n","    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n","    output_ids = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n","    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","\n","    return output_text\n"],"metadata":{"id":"UTifQKdBrsos","executionInfo":{"status":"ok","timestamp":1690970359607,"user_tz":-180,"elapsed":3,"user":{"displayName":"Amanda Hachem","userId":"07936363838080570525"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Test the Function"],"metadata":{"id":"q_TCXsVlr6qo"}},{"cell_type":"markdown","source":["A sample prompt, such as \"Once upon a time,\" is provided to the 'generate_text_with_gpt2()' function, which generates text based on the prompt. The generated text is then displayed as the output."],"metadata":{"id":"25vkXkugr5a4"}},{"cell_type":"code","source":["\n","# Demo\n","prompt = \"Once upon a time\"\n","generated_text = generate_text_with_gpt2(prompt)\n","\n","print(\"Prompt:\")\n","print(prompt)\n","print(\"\\nGenerated Text:\")\n","print(generated_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kwm3J0HWr1al","executionInfo":{"status":"ok","timestamp":1690970399278,"user_tz":-180,"elapsed":39673,"user":{"displayName":"Amanda Hachem","userId":"07936363838080570525"}},"outputId":"ce4449d1-cc65-4382-eeb4-1417f8ba3b12"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Prompt:\n","Once upon a time\n","\n","Generated Text:\n","Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great\n"]}]},{"cell_type":"markdown","source":["In the case of GPT-2, the model generates text by predicting the next token given the preceding tokens. While GPT-2 is a powerful language model, it is not explicitly trained to avoid generating repeated phrases or sentences. As a result, the model can sometimes get stuck in loops and generate repetitive patterns."],"metadata":{"id":"WKWH_UXbqaQS"}}]}