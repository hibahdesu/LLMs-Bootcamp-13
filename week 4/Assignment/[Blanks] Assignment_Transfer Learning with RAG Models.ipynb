{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Transfer Learning with RAG Models\n","----\n","**Objective**: In this notebook, you will experiment with a QA RAG model on the climate_fever dataset using the Haystack framework. You will get to go through the whole process of fitting the parts of a RAG model together, and learn how to prompt it with queries to get answers from the provided dataset.\n","\n","NOTE: Make sure to change the runtime from CPU to TPU or GPU for faster training"],"metadata":{"id":"43hOmsSmsdsq"}},{"cell_type":"markdown","source":["## Install Libraries\n","Install the Haystack (for colab) and Datasets libraries"],"metadata":{"id":"GFOp78HWQKKX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rLW-gBSXVsvU"},"outputs":[],"source":["!pip install farm-haystack[colab]\n","!pip install datasets"]},{"cell_type":"markdown","source":["## Import Dataset\n","----\n","In this section, we will use as an example the climate_fever dataset. The dataset consists of 1535 rows of claims about climate change, and they either refute or support climate change, with some claims being neutral. We will build with a specific topic in mind so that we can get more accurate answers, and keep in mind that bigger datasets with open topics can also be used."],"metadata":{"id":"cWFb0ReWWK-Z"}},{"cell_type":"markdown","source":["**Question 1**: Use the \"load_dataset\" function to load the \"climate_fever\" dataset with the \"test\" split."],"metadata":{"id":"uf_GSEF9NkHQ"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","# Load the dataset\n"],"metadata":{"id":"GKGpONh5i_3a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Formatting and Writing Documents\n","----\n","First, we need to extract, format and write the documents from our chosen dataset so that we can later build our QA Pipeline. This Pipeline will facilitate the process of building our RAG model and getting answers from it.\n","\n","Keep in mind that for this notebook we will focus on how to build the pipeline with the simplest configurations. Feel free to experiment with different parameters.\n","\n"],"metadata":{"id":"amMMwyd4aWjJ"}},{"cell_type":"markdown","source":["**Question 2**: Use the write_documents method to save the formatted documents into document_storage"],"metadata":{"id":"wUPCVg7DcVXu"}},{"cell_type":"code","source":["from haystack.document_stores import InMemoryDocumentStore\n","\n","# Extract and format the documents from the dataset\n","\n","\n","\n","document_storage = InMemoryDocumentStore(use_bm25=True)\n","\n","# Write the documents to the document store\n","\n"],"metadata":{"id":"AKLcvGBGcSG3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preparing the Retriever\n","----\n","We need to prepare our Retriever node of our pipeline. It will be responsible to get the documents from our document storage, so that they can be used by the Language Model later. We will the BM25Retriever provided by haystack, as it is the recommended Retriever for begginners."],"metadata":{"id":"kEw3rT_Ifu9S"}},{"cell_type":"markdown","source":["**Question 3**: Create the BM25Retriever using the document_storage created earlier, with a top_k of value 2"],"metadata":{"id":"HoeETHTAgiMY"}},{"cell_type":"code","source":["from haystack.nodes import BM25Retriever\n","\n","# Note: The higher the top_k is, the better the answer will be. However, speed will be affected\n","\n"],"metadata":{"id":"-oEBHLR0pIdn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preparing the Language Model\n","----\n","Now, we will prepare our Language Model using the prompt node. We need to first create our prompt, and for that, Haystack requires a specific structure. We will then define our desired language model alongside the prompt template we created. When creating this template, we need to Parse the output to a format that Haystack can use."],"metadata":{"id":"88xvymE1hGoi"}},{"cell_type":"markdown","source":["**Question 4**: Define the prompt node using PromptNode with the model name as \"google/flan-t5-large\" and the default prompt template as the created \"rag_prompt\""],"metadata":{"id":"7rcCzztZiGYw"}},{"cell_type":"code","source":["from haystack.nodes import PromptNode, PromptTemplate, AnswerParser\n","\n","rag_prompt = PromptTemplate(\n","    prompt=\"\"\"Create comprehensive answers from the related text given the questions.\n","                             Provide a clear and concise response that displays the key points and information presented in the related text.\n","                             Your answer should be in your own words and be no longer than 50 words.\n","                             \\n\\n Related text: {join(documents)} \\n\\n Question: {query} \\n\\n Answer:\"\"\",\n","    output_parser=AnswerParser(),\n",")\n","\n","prompt_node = ----------------------------"],"metadata":{"id":"y8H34FFcpPtf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fitting our Pipeline Together\n","----\n","Finally, we are going to put our pipeline nodes together. For that we will use the Pipeline function from haystack. With the pipeline ready you will be able to ask it questions and get answers"],"metadata":{"id":"etr1NjR-i-Vm"}},{"cell_type":"markdown","source":["**Question 5**: Add the retriever node and prompt_node created in the previous steps to the Pipeline using the add_node function. Hint: you need to provide the inputs to each of these nodes."],"metadata":{"id":"Ydu0gj7ojjEp"}},{"cell_type":"code","source":["from haystack.pipelines import Pipeline\n","\n","pipe = Pipeline()\n","pipe.add_node(-----------------)\n","pipe.add_node(-----------------)"],"metadata":{"id":"VtgT2ZjNpUka"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Asking the RAG Model Questions\n","----\n","We use the pipeline .run() method to ask a question. Since the output provided by our Prompt Node is a Haystack object, we retrieve in the way provided inside the print() function."],"metadata":{"id":"gLS6VxZcwUFt"}},{"cell_type":"code","source":["output = pipe.run(query=\"When did global warming start\")\n","\n","print(output[\"answers\"][0].answer)"],"metadata":{"id":"JjT-ORZKpWkL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Here are some other examples you can use\n","examples = [\n","    \"Who is most responsible for pollution\",\n","    \"What is the biggest damaging factor for the climate?\",\n","    \"What are some clean energy sources?\",\n","    \"How much does the average temperature of our planet rise per decade?\"\n","]"],"metadata":{"id":"y1QY-cruqFwf"},"execution_count":null,"outputs":[]}]}